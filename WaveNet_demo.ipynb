{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook gives a quick overview of this WaveNet implementation, i.e. creating the model and the data set, training the model and generating samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovis/Documents/pytorch-wavenet/venv/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This is an implementation of WaveNet as it was described in the original paper (https://arxiv.org/abs/1609.03499). Each layer looks like this:\n",
    "\n",
    "```\n",
    "            |----------------------------------------|      *residual*\n",
    "            |                                        |\n",
    "            |    |-- conv -- tanh --|                |\n",
    " -> dilate -|----|                  * ----|-- 1x1 -- + -->  *input*\n",
    "                 |-- conv -- sigm --|     |\n",
    "                                         1x1\n",
    "                                          |\n",
    " ---------------------------------------> + ------------->  *skip*\n",
    "```\n",
    "\n",
    "Each layer dilates the input by a factor of two. After each block the dilation is reset and start from one. You can define the number of layers in each block (``layers``) and the number of blocks (``blocks``). The blocks are followed by two 1x1 convolutions and a softmax output function.\n",
    "Because of the dilation operation, the independent output for multiple successive samples can be calculated efficiently. With ``output_length``, you can define the number these outputs. Empirically, it seems that a large number of skip channels is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  7\n",
      "parameter count:  820384\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=2,\n",
    "                     blocks=2,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16,\n",
    "                     bias=True)\n",
    "# model = load_latest_model_from('snapshots')\n",
    "\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create the data set, you have to specify a path to a data set file. If this file already exists it will be used, if not it will be generated. If you want to generate the data set file (a ``.npz`` file), you have to specify the directory (``file_location``) in which all the audio files you want to use are located. The attribute ``target_length`` specifies the number of successive samples are used as a target and corresponds to the output length of the model. The ``item_length`` defines the number of samples in each item of the dataset and should always be ``model.receptive_field + model.output_length - 1``.\n",
    "\n",
    "```\n",
    "          |----receptive_field----|\n",
    "                                |--output_length--|\n",
    "example:  | | | | | | | | | | | | | | | | | | | | |\n",
    "target:                           | | | | | | | | | |  \n",
    "```\n",
    "To create a test set, you should define a ``test_stride``. Then each ``test_stride``th item will be assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot input\n",
      "the dataset has 215994 items\n"
     ]
    }
   ],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/test_dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      #file_location='bowie_wav',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Logging\n",
    "This implementation supports logging with TensorBoard (you need to have TensorFlow installed). You can even generate audio samples from the current snapshot of the model during training. This will happen in a background thread on the cpu, so it will not interfere with the actual training but will be rather slow. If you don't have TensorFlow, you can use the standard logger that will print out to the console.\n",
    "The trainer uses Adam as default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_log_samples(step):\n",
    "    sample_length=320\n",
    "    gen_model = load_latest_model_from('snapshots')\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    \n",
    "    logger.audio_summary('temperature_0.5', samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    logger.audio_summary('temperature_1.0', samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "logger = TensorboardLogger(log_interval=200,\n",
    "                           validation_interval=400,\n",
    "                           generate_interval=1000,\n",
    "                           generate_function=generate_and_log_samples,\n",
    "                           log_dir=\"logs/test_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n",
      "one training step does take approximately 0.06699719905853271 seconds)\n",
      "load model snapshots/chaconne_model_2017-12-28_16-44-12\n",
      "start generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovis/Documents/pytorch-wavenet/venv/lib/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'wavenet_model.WaveNetModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/lovis/Documents/pytorch-wavenet/venv/lib/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/lovis/Documents/pytorch-wavenet/venv/lib/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one generating step does take approximately 0.0718166732788086 seconds)\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "one generating step does take approximately 0.062008402347564696 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-26-53\n",
      "start generating...\n",
      "one generating step does take approximately 0.012911627292633057 seconds)\n",
      "one generating step does take approximately 0.016660680770874025 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-26-53\n",
      "start generating...\n",
      "one generating step does take approximately 0.012651748657226562 seconds)\n",
      "one generating step does take approximately 0.011625595092773437 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-29-34\n",
      "start generating...\n",
      "one generating step does take approximately 0.009312989711761475 seconds)\n",
      "one generating step does take approximately 0.014244542121887208 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-29-34\n",
      "start generating...\n",
      "one generating step does take approximately 0.01482250213623047 seconds)\n",
      "one generating step does take approximately 0.010836009979248048 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-32-15\n",
      "start generating...\n",
      "one generating step does take approximately 0.02079042911529541 seconds)\n",
      "one generating step does take approximately 0.011225812435150147 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-32-15\n",
      "start generating...\n",
      "one generating step does take approximately 0.014142465591430665 seconds)\n",
      "one generating step does take approximately 0.011355934143066406 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-37-30\n",
      "start generating...\n",
      "one generating step does take approximately 0.02304675579071045 seconds)\n",
      "one generating step does take approximately 0.013163883686065674 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-37-30\n",
      "start generating...\n",
      "one generating step does take approximately 0.014070310592651368 seconds)\n",
      "one generating step does take approximately 0.014678916931152345 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-46-12\n",
      "start generating...\n",
      "one generating step does take approximately 0.020506856441497804 seconds)\n",
      "one generating step does take approximately 0.015232949256896973 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-46-12\n",
      "start generating...\n",
      "one generating step does take approximately 0.0146819806098938 seconds)\n",
      "one generating step does take approximately 0.012917098999023437 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-54-12\n",
      "start generating...\n",
      "one generating step does take approximately 0.013523607254028321 seconds)\n",
      "one generating step does take approximately 0.01576509952545166 seconds)\n",
      "audio clips generated\n",
      "load model snapshots/test_model_2020-05-15_07-55-14\n",
      "start generating...\n",
      "one generating step does take approximately 0.01646632671356201 seconds)\n",
      "one generating step does take approximately 0.019600489139556886 seconds)\n",
      "audio clips generated\n"
     ]
    }
   ],
   "source": [
    "trainer = WavenetTrainer(model=model,\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='test_model',\n",
    "                         snapshot_interval=2000,\n",
    "                         logger=logger)\n",
    "\n",
    "print('start training...')\n",
    "trainer.train(batch_size=16,\n",
    "              epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "This model has the Fast Wavenet Generation Algorithm (https://arxiv.org/abs/1611.09482) implemented. This might run faster on the cpu. You can give some starting data (of at least the length of receptive field) or let the model generate from zero. In my experience, a temperature between 0.5 and 1.0 yields the best results, but this may depend on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% generated\n",
      "one generating step does take approximately 0.01148219347000122 seconds)\n",
      "61% generated\n"
     ]
    }
   ],
   "source": [
    "start_data = data[25000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "generated = model.generate_fast(num_samples=1600,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "torch-wavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
